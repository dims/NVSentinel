# Copyright (c) 2025, NVIDIA CORPORATION.  All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Global configuration settings applied across all NVSentinel components
global:
  # Docker image configuration
  image:
    # Image tag to use for all NVSentinel components (e.g., "main", "v1.0.0")
    tag: "main"

  # Default port for Prometheus metrics endpoint across all components
  metricsPort: 2112

  # Datastore configuration - supports pluggable backends (mongodb, postgresql)
  # Environment-specific values files can override these defaults
  datastore:
    provider: "mongodb"  # Default to MongoDB for backward compatibility
    connection:
      # host is now dynamically determined by nvsentinel.mongodb.serviceName helper
      port: 27017
      database: "nvsentinel"
      # Optional fields (can be overridden in environment-specific values files):
      # username: ""
      # sslmode: ""  # For PostgreSQL: disable, require, verify-ca, verify-full
      # sslcert: ""
      # sslkey: ""
      # sslrootcert: ""

  # Node selector for global components - used to constrain pods to nodes with specific labels
  # Example: kubernetes.io/hostname: specific-node
  nodeSelector: {}

  # Tolerations for global workloads - allows pods to schedule on nodes with matching taints
  # Example: - key: "nvidia.com/gpu", operator: "Exists", effect: "NoSchedule"
  tolerations: []

  # Affinity rules for global workloads - defines pod scheduling preferences and requirements
  # Example: podAntiAffinity to spread pods across nodes
  affinity: {}

  # Additional annotations to apply to all pods
  # Example: prometheus.io/scrape: "true"
  podAnnotations: {}

  # Node selector for control plane components (platform-connectors, monitoring services)
  # Used to schedule critical infrastructure components on specific nodes
  systemNodeSelector: {}

  # Tolerations for system-level components
  # Allows system pods to run on tainted nodes where regular workloads cannot
  systemNodeTolerations: []

  # DCGM (Data Center GPU Manager) integration configuration
  dcgm:
    # Enable DCGM Kubernetes service for GPU metrics collection
    dcgmK8sServiceEnabled: true
    service:
      # DCGM service endpoint in the cluster
      endpoint: "nvidia-dcgm.gpu-operator.svc"
      # DCGM service port
      port: 5555

  # Enable dry-run mode across all components - operations are logged but not executed
  dryRun: false

  # GPU Health Monitor configuration - monitors GPU health via DCGM and system metrics
  gpuHealthMonitor:
    # Enable GPU health monitoring component
    enabled: true

    image:
      # Container image repository for GPU health monitor
      repository: ghcr.io/nvidia/nvsentinel-gpu-health-monitor
      # Image pull policy (IfNotPresent, Always, Never)
      pullPolicy: IfNotPresent

    # Use host networking for GPU health monitor pods
    # Required for accessing host-level GPU metrics
    useHostNetworking: false

    # BusyBox image used for init containers and helper tasks
    busybox:
      image:
        repository: public.ecr.aws/docker/library/busybox
        tag: "1.37.0"
        pullPolicy: IfNotPresent

  # Health Events Analyzer - analyzes historic health events and determines remediation actions
  healthEventsAnalyzer:
    # Enable health events analyzer component
    enabled: false
    image:
      # Container image repository for health events analyzer
      repository: ghcr.io/nvidia/nvsentinel-health-events-analyzer
      pullPolicy: IfNotPresent

  # Fault Quarantine Module - isolates faulty nodes to prevent cascading failures
  faultQuarantineModule:
    # Enable fault quarantine module
    enabled: false

    image:
      # Container image repository for fault quarantine module
      repository: ghcr.io/nvidia/nvsentinel-fault-quarantine-module
      pullPolicy: IfNotPresent

  # Node Drainer Module - gracefully drains nodes before maintenance or replacement
  nodeDrainerModule:
    # Enable node drainer module
    enabled: false

    image:
      # Container image repository for node drainer module
      repository: ghcr.io/nvidia/nvsentinel-node-drainer-module
      pullPolicy: IfNotPresent

  # Fault Remediation Module - executes remediation workflows for detected faults
  faultRemediationModule:
    # Enable fault remediation module
    enabled: false

    image:
      # Container image repository for fault remediation module
      repository: ghcr.io/nvidia/nvsentinel-fault-remediation-module
      pullPolicy: IfNotPresent

  # Janitor - performs cleanup and maintenance tasks for cluster resources
  janitor:
    # Enable janitor module
    enabled: false

    image:
      # Container image repository for janitor module
      repository: ghcr.io/nvidia/nvsentinel-janitor
      pullPolicy: IfNotPresent

  # CSP Health Monitor - monitors cloud service provider specific health metrics
  cspHealthMonitor:
    # Enable CSP health monitor component
    enabled: false

    image:
      # Container image repository for CSP health monitor
      repository: ghcr.io/nvidia/nvsentinel-csp-health-monitor
      # Image pull policy (IfNotPresent, Always, Never)
      pullPolicy: IfNotPresent

  # Syslog Health Monitor - monitors system logs for hardware errors and failures
  syslogHealthMonitor:
    # Enable syslog health monitor
    enabled: true

    image:
      # Container image repository for syslog health monitor
      repository: ghcr.io/nvidia/nvsentinel-syslog-health-monitor
      # Image pull policy (IfNotPresent, Always, Never)
      pullPolicy: IfNotPresent

    # XID error detection sidecar configuration
    xidSideCar:
      # Enable XID error detection sidecar component
      enabled: false

  # Labeler Module - applies labels to nodes based on health status and characteristics
  labeler:
    # Enable labeler module
    enabled: true

    image:
      # Container image repository for labeler module
      repository: ghcr.io/nvidia/nvsentinel-labeler-module
      # Image pull policy (IfNotPresent, Always, Never)
      pullPolicy: IfNotPresent

  # Image pull secrets for accessing private container registries
  # Used to authenticate with NVIDIA GPU Cloud (NGC) registry
  imagePullSecrets:
    - name: nvidia-ngcuser-pull-secret

  # MongoDB Store configuration - utility images for MongoDB operations
  mongodbStore:
    # Utility images for MongoDB management and operations
    images:
      # Kubectl image for Kubernetes operations in MongoDB setup
      kubectl:
        repository: docker.io/bitnamilegacy/kubectl
        tag: "1.30.6"
        pullPolicy: IfNotPresent
      # MongoDB shell image for database operations and maintenance
      mongosh:
        repository: ghcr.io/rtsp/docker-mongosh
        tag: "2.5.2"
        pullPolicy: IfNotPresent

  # In-cluster File Server - provides file serving capabilities within the cluster
  inclusterFileServer:
    # Enable in-cluster file server component
    enabled: false
    # Metrics port for file server metrics endpoint
    metricsPort: 9001
    # Metrics port for cleanup service metrics endpoint
    cleanupMetricsPort: 9002

  # Kata Containers integration - secure container runtime using lightweight VMs
  kata:
    # Enable Kata Containers integration
    enabled: false

# Platform Connector configuration - core component that interfaces with various platform services
platformConnector:
  # Container image configuration
  image:
    # Container image repository for platform connectors
    repository: ghcr.io/nvidia/nvsentinel-platform-connectors
    # Image pull policy (IfNotPresent, Always, Never)
    pullPolicy: IfNotPresent

  # Resource limits and requests for platform connector pods
  resources:
    limits:
      # Maximum CPU allocation
      cpu: 200m
      # Maximum memory allocation
      memory: 512Mi
    requests:
      # Requested CPU allocation
      cpu: 200m
      # Requested memory allocation
      memory: 512Mi

  # Security context for platform connector containers
  securityContext:
    # Run as root user (required for certain platform operations)
    runAsUser: 0
    capabilities:
      # Drop all capabilities for enhanced security
      drop:
      - ALL

  # Prometheus metrics endpoint port
  metricsPort: 2112

  # Log level for platform connector (debug, info, warn, error)
  logLevel: info

  # MongoDB store configuration for platform connector
  mongodbStore:
    # Enable MongoDB store integration
    enabled: false
    # Mount path for MongoDB client certificates
    clientCertMountPath: "/etc/ssl/mongo-client"

  # Pod security context configuration (empty by default)
  podSecurityContext: {}

  # Deployment update strategy
  updateStrategy: RollingUpdate
  # Maximum percentage of unavailable pods during updates
  maxUnavailable: 5%

  # Kubernetes connector configuration
  k8sConnector:
    # Enable Kubernetes connector component
    enabled: true
    # Queries per second rate limit for Kubernetes API calls
    qps: 5.0
    # Burst limit for Kubernetes API calls
    burst: 10

# Helm chart name overrides
# Override for the chart name (defaults to chart name if empty)
nameOverride: ""
# Override for the full resource names (defaults to release name + chart name if empty)
fullnameOverride: ""

# Service account configuration for NVSentinel components
serviceAccount:
  # Create a service account for NVSentinel (recommended)
  create: true
  # Additional annotations to apply to the service account
  annotations: {}
  # Service account name (defaults to fullname template if empty)
  name: ""

# Additional annotations to apply to NVSentinel pods
# Example: prometheus.io/scrape: "true"
podAnnotations: {}

# Node selector for pod placement - used to constrain pods to nodes with specific labels
# Example: kubernetes.io/hostname: specific-node
nodeSelector: {}

# Tolerations for pod scheduling - allows pods to schedule on nodes with matching taints
# Example: - key: "nvidia.com/gpu", operator: "Exists", effect: "NoSchedule"
tolerations: []

# Affinity rules for pod scheduling - defines pod placement preferences and requirements
# Example: podAntiAffinity to spread pods across nodes
affinity: {}

# Socket path for NVSentinel component communication
# Used for inter-component communication within the cluster
socketPath: "/var/run/nvsentinel.sock"
